## 告别hello world的cuda程序


### 数组相加

cuda可用于高效率的数组相加，先来看一个CPU的数组相加的例子：

```cpp
#include <math.h>
#include <stdlib.h>
#include <stdio.h>

const double EPSILON = 1.0e-15;
const double a = 1.23;
const double b = 2.34;
const double c = 3.57;
void add(const double* x, const double* y, double* z, const int N);
void check(const double* z, const int N);

int main(void) {
	const int N = 100000000;
	const int M = sizeof(double) * N;
	double* x = (double*) malloc(M);
	double* y = (double*) malloc(M);
	double* z = (double*) malloc(M);

	for (int n = 0; n < N; ++n) {
		x[n] = a;
		y[n] = b;
	}

	add(x, y, z, N);
	check(z, N);

	free(x);
	free(y);
	free(z);
	
	return 0;
}

void add(const double* x, const double* y, double* z, const int N) {
	for (int n = 0; n < N; ++n) {
		z[n] = x[n] + y[n];
	}
}

void check(const double* z, const int N) {
	bool has_error = false;
	for (int n = 0; n < N; ++n) {
		if (fabs(z[n] - c) > EPSILON) {
			has_error = true;
		}
	}
	printf("%s\n", has_error ? "Has errors!" : "No errors!");
}

```

> 这个程序的运行需要2.4G内存，如果内存不够可以适当调节参数。

一般只有一个源文件的情况下，cuda程序的框架如下：

```cpp
头文件包含
常量定义（或者宏定义）
C++ 自定义函数和CUDA核函数的声明（原型）
int main(void) {
	分配主机与设备内存
	初始化主机中的数据
	将某些数据从主机复制到设备
	调用核函数在设备中进行计算
	将某些数据从设备复制到主机
	释放主机与设备内存
}
C++ 自定义函数和CUDA核函数的定义（实现）
```
下面是使用核函数进行数组相加：

```cpp
#include <math.h>
#include <stdio.h>

const double EPSILON = 1.0e-15;
const double a = 1.23;
const double b = 2.34;
const double c = 3.57;
void __global__ add(const double* x, const double* y, double* z);
void check(const double* z, const int N);

int main(void) {
	const int N = 100000000;
	const int M = sizeof(double) * N;
	double* h_x = (double*) malloc(M);
	double* h_y = (double*) malloc(M);
	double* h_z = (double*) malloc(M);

	for (int n = 0; n < N; ++n) {
		h_x[n] = a;
		h_y[n] = b;
	}

	double* d_x, *d_y, *d_z;
	cudaMalloc((void**)&d_x, M);
	/* 双重指针，相当于修改了d_x本身的值
	忽略(void**)进行强制类型转换的效果（实际上确实可以不加强制类型转换，函数默认进行类型转换）在cpu中的效果相当于：
	double** dd_x = &d_x;
	*dd_x = (double*) malloc(M);
	 */

	cudaMalloc((void**)&d_y, M);
	cudaMalloc((void**)&d_z, M);
	cudaMemcpy(d_x, h_x, M, cudaMemcpyHostToDevice);
	cudaMemcpy(d_y, h_y, M, cudaMemcpyHostToDevice);

	const int block_size = 128;
	const int grid_size = (N + block_size - 1) / block_size;  // 保证不越界
	add << <grid_size, block_size >> > (d_x, d_y, d_z);
	cudaMemcpy(h_z, d_z, M, cudaMemcpyDeviceToHost);
	check(h_z, N);

	free(h_x);
	free(h_y);
	free(h_z);
	cudaFree(d_x);
	cudaFree(d_y);
	cudaFree(d_z);
	
	return 0;
}

void __global__ add(const double* x, const double* y, double* z) {
	const int n = blockDim.x * blockIdx.x + threadIdx.x;
	z[n] = x[n] + y[n];
}

void check(const double* z, const int N) {
	bool has_error = false;
	for (int n = 0; n < N; ++n) {
		if (fabs(z[n] - c) > EPSILON) {
			has_error = true;
		}
	}
	printf("%s\n", has_error ? "Has errors!" : "No errors!");
}

```

这里介绍几个CUDA运行时API函数：

```cpp
cudaError_t cudaMalloc(void** address, size_t size);
```
该函数的第一个参数是待分配内存指针的地址，第二个参数是待分配的内存大小，返回值是一个错误代号。如果运行成功，则返回`cudaSuccess`，否则返回相应的错误代号。

```cpp
cudaError_t cudaFree(void* address);
```
该函数用来释放内存资源。

```cpp
cudaError_t cudaMemcpy(
	void* dst,  // 目标地址
	const void* src,  // 源地址
	size_t const,  // 复制数据的字节数
	enum cudaMemcpyKind kind  // 数据传递方向的类型
);
```
其中第四个参数有5种，分别是`cudaMemcpyHostToHost`，`cudaMemcpyHostToDevice`，`cudaMemcpyDeviceToDevice`，`cudaMemcpyDeviceTOHost`，`cudaMemcpyDefault`，最后一种是自动判断传输方向，要求是64位主机。

cuda允许自定义设备函数，使用`__device__`声明。核函数是主机调用，设备执行，而自定义设备函数是设备调用，设备执行，因此核函数可以调用设备函数，设备函数也可以调用其它设备函数。此外还有`__host__`修饰符，用于声明函数是一个普通的c++函数，可以和`__device__`一起使用，声明函数既是c++函数，也是设备函数，编译器在编译时就不用分别编译一个设备函数，一个主机函数了。下面是一个示例：

```cpp
// 有返回值的设备函数
double __device__ add1_device(const double x, const double y) {
	return (x + y);
}

void __global__ add1(const double* x, const double* y, double* z, const int N) {
	const int n = blockDim.x * blockIdx.x + threadIdx.x;
	if (n < N) {
		z[n] = add1_device(x[n], y[n]);
	}
}

// 用指针的设备函数
void __device__ add2_device(const double x, const double y, double* z) {
	*z = x + y;
}

void __global__ add2(const double* x, const double* y, double* z, const int N) {
	const int n = blockDim.x * blockIdx.x + threadIdx.x;
	if (n < N) {
		add2_device(x[n], y[n], &z[n]);
	}
}

// 用引用的设备函数
void __device__ add3_device(const double x, const double y, double& z) {
	z = x + y;
}

void __global__ add3(const double* x, const double* y, double* z, const int N) {
	const int n = blockDim.x * blockIdx.x + threadIdx.x;
	if (n < N) {
		add3_device(x[n], y[n], z[n]);
	}
}
```

### 简单cuda程序模板

```cpp
#include <stdio.h>
#include <stdlib.h>
#include <cuda_runtime.h>

__global__ void VectorAdd(const float* A, const float* B, float* C, int Row, int Col) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    int idx = row * Col + col; // 转换为线性索引

    if (row < Row && col < Col) {
        C[idx] = A[idx] + B[idx];
    }
}


void init(float *a, int N) {
    for (int i = 0; i < N; ++i)
        a[i] = rand() % 100;
}

int main() {
    int Row = 1024;
    int Col = 1024;
    int N = Row * Col;
    float *a, *b, *c;
    a = new float[N];
    b = new float[N];
    c = new float[N];

    float *da, *db, *dc;
    cudaMalloc(&da, sizeof(float) * N);
    cudaMalloc(&db, sizeof(float) * N);
    cudaMalloc(&dc, sizeof(float) * N);

    init(a, N);
    init(b, N);

    cudaMemcpy(da, a, sizeof(float) * N, cudaMemcpyHostToDevice);
    cudaMemcpy(db, b, sizeof(float) * N, cudaMemcpyHostToDevice);

    // cuda核函数调用部分
    dim3 block(32, 32);
    dim3 grid((Row + block.x - 1) / block.x, (Col + block.y - 1) / block.y);
    VectorAdd2<<<grid, block>>>(da, db, dc, Row, Col);
    // 调用结束


    cudaError_t error = cudaGetLastError();
    if(error != cudaSuccess) {
        printf("%s:\n", cudaGetErrorString(error));
        exit(1);
    }

    cudaMemcpy(c, dc, sizeof(float) * N, cudaMemcpyDeviceToHost);

    delete[] a;
    delete[] b;
    delete[] c;

    cudaFree(da);
    cudaFree(db);
    cudaFree(dc);

    return 0;
}
```